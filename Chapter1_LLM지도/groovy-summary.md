### 딥러닝과 언어 모델링

- 딥러닝
    - 인간의 두뇌에 영감을 받아 만들어진 신경망, 데이터의 패턴을 학습하는 머신러닝의 한분야이다.
    - 표 형태의 정형 데이터뿐만 아니라, 텍스트와 이미지 같은 비정형 데이터도 다룰 수 있음.
    - LLM은 자연어 처리(natural language processing) 분야에 속하며, 사람과 비슷하게 텍스트를 생성하는 방법을 연구하는 자연어 생성에 속한다.
    - 다음에 올 단어를 예측하는 모델을 **언어 모델** 이라고 한다.
    - LLM은 딥러닝 기반의 언어 모델이다.
- 딥러닝이 문제를 해결하는 방법
    - 문제의 유형(자연어, 이미지 처리)에 따라 일반적으로 사용하는 모델을 준비
    - 풀고자 하는 문제에 대한 학습 데이터 준비
    - 학습 데이터를 반복적으로 모델에 입력
- 딥러닝과 머신 러닝의 가장 큰 차이점
    - “데이터의 특징을 누가 뽑는가?”
    - 머신 러닝의 경우, 개발자가 직접 데이터의 특징을 입력해야 하지만 딥러닝의 경우 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.

---

### 임베딩: 딥러닝 모델이 데이터를 표현하는 방식

- 임베딩
    - 데이터의 의미와 특징을 포착해 숫자로 표현할 것
    - 임베딩은 숫자로 거리를 계산하여 유사도를 측정하는 데 사용한다.
        - 검색 및 추천: 검색어와 관련 있는 상품 추천
        - 클러스터링 및 분류: 유사하고 관련 있는 데이터를 하나로 묶기
        - 이상치(outlier) 탐지: 나머지 데이터와 거리가 먼 데이터는 이상치로 보기
- Word2vec
    - 단어를 임베딩으로 변환하는 모델
    - 숫자 하나하나의 의미를 알 순 없지만, 숫자 모음의 경우 그 의미를 가진다.

---

### 언어 모델링: 딥러닝 모델의 언어 학습법

- 언어 모델링
    - 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식
    - 대량의 데이터에서 언어의 특성을 학습하는 pre-training 과제로도 많이 사용
- 전이 학습(transfer learning)
    - 딥러닝에서 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
- 사전 학습(pre-training)
    - 대량의 데이터로 모델을 학습
- 미세 조정(fine-tune)
    - 특정한 문제를 해결하기 위한 데이터로 추가 학습
- [fast.ai](http://fast.ai) 의 제레미 하워드와 세바스찬 루더는 다음 단어를 예측하는 언어 모델링 방식으로 pre-training 을 수행했을 때, 훨씬 적은 레이블 데이터로 기존 지도 학습 모델링의 성능을 뛰어넘는다는 사실을 발견함.
    - pre-training → transfer-learning → fine-tuning

---

### RNN에서 트랜스포머 아키텍처로

- 시퀀스
    - 딥러닝이나 머신러닝 분야에서 텍스트는 단어가 연결된 문장 형태의 데이터를 일컫는다. 이처럼 작은 단위(단어)의 데이터가 연결 되고, 그 길이가 다양한 데이터의 형태 **시퀀스** 라고 한다.
- RNN
    - hidden-state : 잠재상태
    - 입력을 누적하여 hidden-state 에 압축하여 맥락(**context)** 저장
    - 하나의 hidden-state 에 압축하기 때문에, 메모리를 적게 사용하며, 다음 단어를 빠르게 생성 가능. 하지만 먼저 입력한 단어의 의미가 점차 희석, 입력이 길어지면 의미를 충분히 담지 못하며 성능이 떨어진다.
- Transformer
    - RNN 의 순차처리 방식을 버린다. context를 모두 참조하는 **attention 연산** 을 사용
    - 입력 하나하나 hidden-state 로 맥락 압축 및 맥락 데이터를 모두 그대로 활용한다.
    - 높은 성능을 보이지만, 입력이 길어지면 메모리 사용량이 증가한다. 다음 단어를 예측하기 위해 모든 맥락 데이터를 확인해야 하기 때문에 무겁고 비효율적인 연산을 한다.