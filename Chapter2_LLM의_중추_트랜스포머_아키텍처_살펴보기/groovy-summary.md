### 트랜스포머 아키텍처란

*Attention is All you need.*

해당 논문에서 처음 등장한 LLM 아키텍처

- Transformer는 RNN 에 비해 성능만 높은 것이 아닌 모델 학습 속도도 빨랐다,
- 현재 Transfomer는 자연어 처리는 물론 컴퓨터 비전, 추천 시스템 등 모든 AI 분야에서 핵심 아키텍처로 사용되고 있다.
- Self-Attention 개념을 도입하여, 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현(representation)을 조정
- 확장성이 좋다.
    - 더 깊은 모델을 만들어도 학습이 잘되며, 동일한 블럭을 반복해 사용하기 때문에 확장에 용이
- 효율성이 좋다.
    - 학습할 때 병렬 연산이 가능하므로, 학습 시간이 단축
- 더 긴 입력 처리
    - 입력이 길어져도 성능이 거의 떨어지지 않는다.
- 결국 Transformer Architecture가 있었기에, LLM이 가능해졌다.

---

### 텍스트를 임베딩으로 변환하기

- 컴퓨터는 텍스트를 계산에 이용할 수 없기에, 숫자 형식의 데이터로 변경한다.
- 텍스트를 모델에 입력할 수 있는 임베딩으로 변환하기 위해서는 크게 세 가지 과정을 거친다.

- 토큰화 - tokenization
    - 텍스트를 적절한 단위로 잘라 숫자형 아이디(ID)를 부여
- 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자의 집합인 토큰 임베딩으로 변환
- 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 있는 위치 임베딩을 추가해 최종적으로 모델에 입력할 임베딩을 만든다.

---

### 토큰화

- 토큰화를 할 때는 어떤 토큰이 어떤 숫자 아이디로 연결됐는지 기록해 사전(vocabulary)로 만든다.
- 토큰화의 크기가 커질 수록 텍스트 의미가 잘 유지 되지만, 사전이 커진다.
- 단어로 토큰화를 할 경우, 이전에 본 적이 없는 새로운 단어는 사전에 없으므로 처리하지 못하는 Out of Vocabulary 문제가 자주 발생한다.
- 작은 단위로 토큰화를 할 경우, OOV는 면할 수 있지만 텍스트의 의미가 잘 유지되지 않는다.
- 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 **subword** 토큰화 방식을 사용한다.

---

### 토큰 임베딩으로 변환하기

- 딥러닝 모델이 텍스트 데이터를 처리하기 위해서는 입력 토큰과 토큰 사이의 관계를 계산해야 한다.
- 토큰의 의미를 담기 위해서는 최소 2개의 숫자 집합인 벡터(vector) 여야 한다.
- Pytorch가 제공하는 nn.Embedding 클래스를 활용하여 토큰 아이디를 토큰 임베딩으로 변환할 수 있다.
- 입력 토큰 임베딩은 의미를 담은 벡터로 변환된 것이 아니기에, 임베딩 층이 단어의 의미를 담기 위해서 딥러닝 모델이 학습 데이터로 훈련되어야 한다.
- 딥러닝은 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습한다.

---

### 위치 인코딩

- 트랜스포머의 경우, RNN 처럼 입력 순차처리를 하지 않기에, 텍스트의 의미를 잘 담기 위해서 위치 정보를 추가해줘야 한다. 그 역할을 위치 인코딩이 담당한다.
- 절대적 위치 인코딩
    - 수식을 통해 위치 정보를 추가하는 방식이나 임베딩으로 위치 정보를 학습하는 방식 모두 결국 모델로 추론을 수행하는 시점에서는 입력 토큰의 위치에 따라 고정된 임베딩을 더해줌.
    - 간단하게 구현할 수 있지만, 토큰 사이의 상대적인 위치 정보를 활용하지 못하고 학습 데이터에서 보기 어려웠던 긴 텍스트를 추론하는 경우에 성능이 떨어진다.

---

### 어텐션 이해하기

- 어텐션 - 입력한 텍스트에서 어떤 단어가 서로 관련되는지 ‘주의를 기울여’ 파악한다
- 쿼리
    - 입력하는 검색어
- 키
    - 쿼리와 연관 된 문서가 가진 특징
- 값
    - 쿼리와 관련이 깊은 키에 대한 관련된 순으로 정렬된 문서
- 가중치
    - 트랜스포머 아키텍처에서는, 토큰 사이의 관계를 잘 파악할 수 있게 하기 위해 가중치를 도입하여 학습 단계에서 업데이트 되게 한다